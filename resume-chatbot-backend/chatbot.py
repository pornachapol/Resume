import os, re, time, io
from typing import List, Tuple, Optional, Dict, Any
import httpx
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
import fitz  # PyMuPDF
import numpy as np
import logging
from scipy.sparse import hstack

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===================== ENV =====================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
RESUME_URL = os.getenv("RESUME_URL")
ALLOWED_ORIGINS = [o.strip() for o in (os.getenv("ALLOWED_ORIGINS", "")).split(",") if o.strip()]
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-2.0-flash")

if not GOOGLE_API_KEY:
    logger.warning("GOOGLE_API_KEY not set")
else:
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        logger.info("Google AI configured successfully")
    except Exception as e:
        logger.error(f"Failed to configure Google AI: {e}")

# ===================== APP =====================
app = FastAPI(title="Enhanced Resume Chatbot", version="2.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS or ["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===================== Schemas =====================
class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=1000)

class ChatResponse(BaseModel):
    reply: str
    sources: List[Tuple[int, str]] = []
    response_type: str = "mixed"  # factual, opinion, mixed
    confidence: float = 0.0  # Confidence in factual information

# ===================== Enhanced Data Structures =====================
class ResumeChunk:
    def __init__(self, content: str, section: str = "general", metadata: Dict = None):
        self.content = content
        self.section = section
        self.metadata = metadata or {}
        
class ProfileData:
    def __init__(self):
        self.name_en = None
        self.name_th = None
        self.email = None
        self.phone = None
        self.location = None
        self.linkedin = None
        self.github = None
        self.skills = []
        self.experience = []
        self.education = []
        self.strengths = []
        self.achievements = []

class QuestionClassifier:
    """Classify questions to determine appropriate response strategy"""
    
    @staticmethod
    def classify_question(question: str) -> Dict[str, Any]:
        q_lower = question.lower()
        
        # Direct factual questions
        factual_patterns = [
            r'‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£|what.*name',
            r'‡∏≠‡∏µ‡πÄ‡∏°‡∏•|email',
            r'‡πÄ‡∏ö‡∏≠‡∏£‡πå|‡πÇ‡∏ó‡∏£|phone',
            r'‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà|location|address',
            r'‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Å‡∏µ‡πà‡∏õ‡∏µ|years.*experience',
            r'‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏ö|graduated|education',
            r'‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà|work.*at|company'
        ]
        
        # Opinion/analysis questions
        opinion_patterns = [
            r'‡πÄ‡∏´‡∏°‡∏≤‡∏∞|suitable|fit|match',
            r'‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á|‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô|strength|weakness',
            r'‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥|recommend|suggest',
            r'‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤|think|opinion',
            r'‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô|evaluate|assess',
            r'‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö|compare',
            r'‡∏Ñ‡∏ß‡∏£|should|would',
            r'‡πÇ‡∏≠‡∏Å‡∏≤‡∏™|opportunity|potential'
        ]
        
        # Skill/capability questions (semi-factual)
        skill_patterns = [
            r'‡∏ó‡∏±‡∏Å‡∏©‡∏∞|skill|ability|competent',
            r'‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ|can|able',
            r'‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå.*‡πÉ‡∏ô|experience.*in',
            r'‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥|have.*done|worked.*on'
        ]
        
        # Interview simulation questions
        interview_patterns = [
            r'‡∏ó‡∏≥‡πÑ‡∏°.*‡∏™‡∏ô‡πÉ‡∏à|why.*interested',
            r'motivat|‡πÅ‡∏£‡∏á‡∏à‡∏π‡∏á‡πÉ‡∏à',
            r'goal|‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢',
            r'expect.*salary|‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô.*‡∏Ñ‡∏≤‡∏î',
            r'weakness|‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô.*‡∏Ñ‡∏∏‡∏ì',
            r'challenge|‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢'
        ]
        
        question_type = "factual"  # default
        needs_opinion = False
        interview_mode = False
        confidence_threshold = 0.7
        
        for pattern in factual_patterns:
            if re.search(pattern, q_lower):
                question_type = "factual"
                confidence_threshold = 0.9
                break
                
        for pattern in opinion_patterns:
            if re.search(pattern, q_lower):
                question_type = "opinion"
                needs_opinion = True
                confidence_threshold = 0.5
                break
                
        for pattern in skill_patterns:
            if re.search(pattern, q_lower):
                question_type = "capability"
                needs_opinion = True
                confidence_threshold = 0.6
                break
                
        for pattern in interview_patterns:
            if re.search(pattern, q_lower):
                question_type = "interview"
                needs_opinion = True
                interview_mode = True
                confidence_threshold = 0.3
                break
        
        return {
            "type": question_type,
            "needs_opinion": needs_opinion,
            "interview_mode": interview_mode,
            "confidence_threshold": confidence_threshold
        }

# ===================== Global Variables =====================
RESUME_CHUNKS: List[ResumeChunk] = []
PROFILE: ProfileData = ProfileData()
VECTORIZER = None
TFIDF_MATRIX = None
EMB_MATRIX = None
LAST_FETCH_AT = 0

# ===================== Enhanced Parsing =====================
def parse_structured_resume(text: str) -> Tuple[ProfileData, List[ResumeChunk]]:
    """Parse resume with better structure awareness"""
    profile = ProfileData()
    chunks = []
    
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    current_section = "general"
    section_content = []
    
    # Section patterns
    section_patterns = {
        'basic_info': r'(basic information|personal|contact)',
        'summary': r'(professional summary|summary|profile)',
        'skills': r'(skills|expertise|competencies|technical)',
        'experience': r'(professional experience|experience|work history|career)',
        'education': r'(education|academic|qualification)',
        'achievements': r'(achievements|accomplishments|key results)',
        'strengths': r'(strengths|weaknesses)',
        'goals': r'(goals|objectives|future|passion)'
    }
    
    try:
        for line in lines:
            line_lower = line.lower()
            
            # Detect section changes
            new_section = None
            for section, pattern in section_patterns.items():
                if re.search(pattern, line_lower):
                    new_section = section
                    break
            
            if new_section:
                # Save previous section
                if section_content:
                    chunk = ResumeChunk(
                        content='\n'.join(section_content),
                        section=current_section,
                        metadata={'line_count': len(section_content)}
                    )
                    chunks.append(chunk)
                
                current_section = new_section
                section_content = [line]
            else:
                section_content.append(line)
        
        # Save last section
        if section_content:
            chunk = ResumeChunk(
                content='\n'.join(section_content),
                section=current_section,
                metadata={'line_count': len(section_content)}
            )
            chunks.append(chunk)
        
        # Extract profile data safely
        full_text = text.lower()
        
        # Names
        name_match = re.search(r'name \(en\):\s*([^\n]+)', text, re.IGNORECASE)
        if name_match:
            profile.name_en = name_match.group(1).strip()
        
        th_name_match = re.search(r'‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:\s*([^\n]+)', text)
        if th_name_match:
            profile.name_th = th_name_match.group(1).strip()
        
        # Contact info
        email_match = re.search(r'email:\s*([^\s\n]+)', text, re.IGNORECASE)
        if email_match:
            profile.email = email_match.group(1).strip()
            
        phone_match = re.search(r'phone:\s*([^\s\n]+)', text, re.IGNORECASE)
        if phone_match:
            profile.phone = phone_match.group(1).strip()
            
        location_match = re.search(r'location:\s*([^\n]+)', text, re.IGNORECASE)
        if location_match:
            profile.location = location_match.group(1).strip()
        
        # Skills extraction (improved)
        skills_section = next((c.content for c in chunks if c.section == 'skills'), '')
        if skills_section:
            # Extract skills from bullet points and comma-separated lists
            skill_lines = re.findall(r'[‚Ä¢¬∑-]\s*([^\n]+)', skills_section)
            for line in skill_lines:
                skills = re.split(r'[,/|]', line)
                profile.skills.extend([s.strip() for s in skills if s.strip()])
        
        logger.info(f"Successfully parsed resume: {len(chunks)} chunks, profile name: {profile.name_en}")
        return profile, chunks
        
    except Exception as e:
        logger.error(f"Error parsing resume: {e}")
        return profile, chunks

# ===== [‡πÄ‡∏û‡∏¥‡πà‡∏°] enrich metadata =====
for chunk in chunks:
    # basic section name ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î
    chunk.metadata.setdefault('section', chunk.section)

    # ‡πÄ‡∏î‡∏≤ employer/role/dates ‡πÉ‡∏ô section=experience (pattern ‡∏≠‡∏≤‡∏à‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏à‡∏£‡∏¥‡∏á)
    if chunk.section == 'experience':
        m = re.search(r'(?P<company>[^\n]+?)\s+[-‚Äì]\s+(?P<role>[^\(]+?)\s*\((?P<dates>[^)]+)\)', chunk.content)
        if m:
            chunk.metadata.update({
                'employer': m.group('company').strip(),
                'role': m.group('role').strip(),
                'dates': m.group('dates').strip()
            })

    # ‡∏î‡∏∂‡∏á skills ‡∏à‡∏≤‡∏Å bullets/‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
    if chunk.section == 'skills':
        skill_lines = re.findall(r'[‚Ä¢¬∑\-]\s*([^\n]+)', chunk.content)
        skills = []
        for line in skill_lines:
            skills.extend([s.strip() for s in re.split(r'[,/|]', line) if s.strip()])
        if skills:
            # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ö‡∏ö unique
            chunk.metadata['skills'] = sorted(list({s for s in skills}))


# ===================== Enhanced Query Processing =====================
def expand_query_for_context(query: str, question_class: Dict) -> List[str]:
    """Expand query based on question classification"""
    queries = [query]
    query_lower = query.lower()
    
    if question_class["type"] == "opinion":
        # For opinion questions, get broader context
        if 'data' in query_lower:
            queries.extend([
                'data analysis experience',
                'SQL Python analytics',
                'business intelligence reporting',
                'statistical analysis'
            ])
        if 'project' in query_lower:
            queries.extend([
                'project management',
                'team leadership',
                'stakeholder management'
            ])
        if 'management' in query_lower:
            queries.extend([
                'leadership experience',
                'team management',
                'people management'
            ])
    
    elif question_class["type"] == "capability":
        # For capability questions, focus on skills and experience
        queries.extend([
            'technical skills experience',
            'professional experience',
            'achievements results'
        ])
    
    elif question_class["type"] == "interview":
        # For interview questions, get personal insights
        queries.extend([
            'goals objectives motivation',
            'strengths achievements',
            'challenges learning'
        ])
    
    return queries

def multi_query_retrieval(query: str, question_class: Dict, k: int = 5) -> List[Tuple[int, str, float]]:
    """Enhanced retrieval with question-aware strategy"""
    if not RESUME_CHUNKS or not VECTORIZER:
        logger.warning("No resume chunks or vectorizer available")
        return []
    
    all_results = {}  # chunk_idx -> max_score
    
    try:
        queries = expand_query_for_context(query, question_class)
        
        for q in queries:
            # TF-IDF retrieval
            if isinstance(VECTORIZER, tuple):
                vec_word, vec_char = VECTORIZER
                q_word = vec_word.transform([q]) * 0.5
                q_char = vec_char.transform([q]) * 0.5
                qv = hstack([q_word, q_char])
            else:
                qv = VECTORIZER.transform([q])  # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏Å‡πà‡∏≤
            tfidf_scores = cosine_similarity(qv, TFIDF_MATRIX).ravel()
            
            # Embedding retrieval
            emb_scores = np.zeros_like(tfidf_scores)
            if EMB_MATRIX is not None and EMB_MATRIX.size > 0:
                emb_q = embed_texts([q])
                if emb_q is not None and emb_q.size > 0:
                    qn = emb_q / (np.linalg.norm(emb_q, axis=1, keepdims=True) + 1e-12)
                    emb_scores = (EMB_MATRIX @ qn.T).ravel()
            
            # Question-type aware scoring
            for i, (tfidf_score, emb_score) in enumerate(zip(tfidf_scores, emb_scores)):
                hybrid_score = 0.4 * tfidf_score + 0.6 * emb_score
                
                sec = RESUME_CHUNKS[i].section
                md = RESUME_CHUNKS[i].metadata or {}
                
                # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
                if question_class["type"] == "factual" and sec in ['basic_info', 'education', 'experience']:
                    hybrid_score *= 1.3
                elif question_class["type"] in ["opinion", "capability"] and sec in ['achievements', 'experience', 'skills']:
                    hybrid_score *= 1.2
                elif question_class["type"] == "interview" and sec in ['goals', 'strengths', 'summary']:
                    hybrid_score *= 1.2
                
                # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏ï‡∏≤‡∏° metadata keyword
                ql = q.lower()
                if "sap" in ql and any("sap" in s.lower() for s in md.get("skills", [])):
                    hybrid_score *= 1.2
                if ("ngg" in ql or "kubota" in ql) and md.get("employer", "").lower() in ql:
                    hybrid_score *= 1.2
                
                all_results[i] = max(all_results.get(i, 0), hybrid_score)
        
        # Sort and return top k
        sorted_results = sorted(all_results.items(), key=lambda x: x[1], reverse=True)
        
        # ‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á/‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ 0.2
        scores_arr = np.array([s for _, s in sorted_results]) if sorted_results else np.array([0.0])
        cut = float(max(np.percentile(scores_arr, 50), 0.2))
        
        results = []
        for idx, score in sorted_results[:k * 2]:
            if score >= cut:
                results.append((idx, RESUME_CHUNKS[idx].content, float(score)))
            if len(results) >= k:
                break
        
        logger.info(f"Retrieved {len(results)} relevant chunks for {question_class['type']} question")
        return results
        
    except Exception as e:
        logger.error(f"Error in multi_query_retrieval: {e}")
        return []

# ===================== Smart Response Generation =====================
def generate_smart_response(question: str, contexts: List[str], question_class: Dict) -> Tuple[str, str]:
    """Generate intelligent response based on question type and available context"""
    if not contexts:
        return handle_no_context_response(question, question_class)
    
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        
        if question_class["type"] == "factual":
            prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

**‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
- ‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
- ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà"
- ‡∏ï‡∏≠‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà:**
{chr(10).join(contexts)}

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {question}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
"""
            response_type = "factual"
            
        elif question_class["type"] == "opinion" or question_class["type"] == "capability":
            prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Recruiter ‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç

**‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
1. ‡πÅ‡∏¢‡∏Å‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô
2. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
3. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏•‡∏∞‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏û‡∏±‡∏í‡∏ô‡∏≤

**‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:**
üìã **‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà:**
[‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á]

üí° **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô:**
[‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥]

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà:**
{chr(10).join(contexts)}

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {question}

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
"""
            response_type = "mixed"
            
        elif question_class["type"] == "interview":
            prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏ô‡∏≤‡∏° Nachapol

**‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:**
- ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏µ‡πà 1 (‡∏ú‡∏°/‡∏î‡∏¥‡∏â‡∏±‡∏ô)
- ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà
- ‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏á‡∏≤‡∏ô
- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà:**
{chr(10).join(contexts)}

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå:** {question}

*‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà*

‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
"""
            response_type = "interview_simulation"
            
        else:
            # Default mixed response
            prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà:**
{chr(10).join(contexts)}

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {question}

‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
"""
            response_type = "mixed"
        
        # Generate response
        generation_config = genai.types.GenerationConfig(
            temperature=0.3 if question_class["type"] == "factual" else 0.7,
            top_p=0.8,
            top_k=40,
            max_output_tokens=2048,
        )
        
        response = model.generate_content(prompt, generation_config=generation_config)
        answer = (getattr(response, "text", "") or "").strip()
        
        if not answer:
            return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà", response_type
            
        return answer, response_type
        
    except Exception as e:
        logger.error(f"Error in generate_smart_response: {e}")
        return "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà", "error"

def handle_no_context_response(question: str, question_class: Dict) -> Tuple[str, str]:
    """Handle cases where no relevant context is found"""
    
    if question_class["type"] == "factual":
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô", "no_context"
    
    elif question_class["type"] == "interview":
        # Try to give a general interview-style response
        model = genai.GenerativeModel(MODEL_NAME)
        prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô ‡πÅ‡∏ï‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà

‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {question}

*‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà*
"""
        try:
            response = model.generate_content(prompt)
            answer = getattr(response, "text", "").strip()
            return answer or "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ", "general_interview"
        except:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà", "no_context"
    
    else:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô", "no_context"

# ===================== Profile-based Quick Answers =====================
def get_quick_answer(question: str) -> Optional[Tuple[str, str]]:
    """Quick answers for basic profile questions with response type"""
    try:
        q_lower = question.lower().replace(" ", "")
        
        # Name questions
        if any(k in q_lower for k in ["‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "name", "fullname"]):
            if "thai" in q_lower or "‡πÑ‡∏ó‡∏¢" in q_lower:
                if PROFILE.name_th:
                    return f"‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: {PROFILE.name_th}", "factual"
            elif PROFILE.name_en:
                result = f"‡∏ä‡∏∑‡πà‡∏≠: {PROFILE.name_en}"
                if PROFILE.name_th:
                    result += f" (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: {PROFILE.name_th})"
                return result, "factual"
        
        # Contact info
        if "email" in q_lower or "‡∏≠‡∏µ‡πÄ‡∏°‡∏•" in q_lower:
            if PROFILE.email:
                return f"‡∏≠‡∏µ‡πÄ‡∏°‡∏•: {PROFILE.email}", "factual"
            
        if "phone" in q_lower or "‡πÇ‡∏ó‡∏£" in q_lower:
            if PROFILE.phone:
                return f"‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£: {PROFILE.phone}", "factual"
            
        if "location" in q_lower or "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà" in q_lower:
            if PROFILE.location:
                return f"‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á: {PROFILE.location}", "factual"
        
        return None
        
    except Exception as e:
        logger.error(f"Error in get_quick_answer: {e}")
        return None

# ===================== Utility Functions =====================
def embed_texts(texts: List[str]) -> np.ndarray:
    """Embed ‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (loop) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ batch schema ‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô"""
    if not texts:
        return np.zeros((0, 1), dtype="float32")
    vecs = []
    try:
        for t in texts:
            resp = genai.embed_content(
                model="text-embedding-004",
                content=t,
                task_type="retrieval_document"
            )
            emb = resp.get("embedding") or resp.get("embeddings")
            if isinstance(emb, dict) and "values" in emb:
                vecs.append(np.array(emb["values"], dtype="float32"))
            elif isinstance(emb, list) and len(emb) > 0 and isinstance(emb[0], dict) and "values" in emb[0]:
                vecs.append(np.array(emb[0]["values"], dtype="float32"))
            else:
                # fallback ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏∏‡∏î schema
                vecs.append(np.zeros((1024,), dtype="float32"))
        return np.vstack(vecs)
    except Exception as e:
        logger.error(f"Error in embed_texts: {e}")
        return np.zeros((len(texts), 1), dtype="float32")

def calculate_response_confidence(contexts: List[str], question: str, question_class: Dict) -> float:
    """Calculate confidence score for the response"""
    if not contexts:
        return 0.0
    
    # Base confidence from context quality
    context_length = sum(len(c.split()) for c in contexts)
    base_confidence = min(context_length / 100, 1.0)  # Normalize by expected context length
    
    # Adjust by question type
    if question_class["type"] == "factual":
        # Factual questions need precise information
        confidence_multiplier = 1.0
    elif question_class["type"] == "opinion":
        # Opinion questions are inherently less certain
        confidence_multiplier = 0.7
    elif question_class["type"] == "interview":
        # Interview simulations are moderate confidence
        confidence_multiplier = 0.8
    else:
        confidence_multiplier = 0.6
    
    return base_confidence * confidence_multiplier

async def fetch_resume_text() -> str:
    """Fetch resume content from URL with better error handling"""
    if not RESUME_URL:
        logger.warning("No RESUME_URL provided")
        return ""
        
    headers = {"User-Agent": "resume-bot/1.0"}
    try:
        async with httpx.AsyncClient(timeout=60) as client:
            r = await client.get(RESUME_URL, follow_redirects=True, headers=headers)
            r.raise_for_status()
            
            content_type = (r.headers.get("Content-Type") or "").lower()
            
            if "application/pdf" in content_type or RESUME_URL.lower().endswith(".pdf"):
                doc = fitz.open(stream=r.content, filetype="pdf")
                pages = []
                for page in doc:
                    text = page.get_text()
                    if text.strip():
                        pages.append(text)
                text_content = "\n".join(pages)
                logger.info(f"Successfully fetched PDF content: {len(text_content)} characters")
                return text_content
            else:
                soup = BeautifulSoup(r.text, "html.parser")
                text_content = soup.get_text()
                logger.info(f"Successfully fetched HTML content: {len(text_content)} characters")
                return text_content
                
    except Exception as e:
        logger.error(f"Error fetching resume: {e}")
        return ""

def build_search_index():
    """Build TF-IDF (word + char) ‡πÅ‡∏•‡∏∞ embeddings ‡∏û‡∏£‡πâ‡∏≠‡∏° normalization"""
    global VECTORIZER, TFIDF_MATRIX, EMB_MATRIX

    if not RESUME_CHUNKS:
        logger.warning("No resume chunks to index")
        return

    try:
        contents = [chunk.content for chunk in RESUME_CHUNKS]

        # 1) Word n-gram (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©/‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå)
        vec_word = TfidfVectorizer(min_df=1, ngram_range=(1, 2), max_features=20000)
        X_word = vec_word.fit_transform(contents)

        # 2) Char n-gram (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢/‡∏ï‡∏¥‡∏î‡∏Ñ‡∏≥)
        vec_char = TfidfVectorizer(analyzer="char_wb", ngram_range=(3, 6), min_df=1, max_features=30000)
        X_char = vec_char.fit_transform(contents)

        # ‡∏£‡∏ß‡∏°‡∏™‡∏≠‡∏á‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á (‡∏ñ‡πà‡∏ß‡∏á‡πÄ‡∏ó‡πà‡∏≤ ‡πÜ ‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô)
        VECTORIZER = (vec_word, vec_char)
        TFIDF_MATRIX = hstack([X_word * 0.5, X_char * 0.5])

        # 3) Embeddings + L2 normalize
        EMB_MATRIX = embed_texts(contents)
        if EMB_MATRIX is not None and EMB_MATRIX.size > 0:
            norms = np.linalg.norm(EMB_MATRIX, axis=1, keepdims=True) + 1e-12
            EMB_MATRIX = EMB_MATRIX / norms

        logger.info(f"Built search index with {len(contents)} chunks (word+char TFIDF, embeddings)")

    except Exception as e:
        logger.error(f"Error building search index: {e}")


async def ensure_data_loaded(force: bool = False):
    """Ensure resume data is loaded and indexed with better error handling"""
    global PROFILE, RESUME_CHUNKS, LAST_FETCH_AT
    
    if RESUME_CHUNKS and not force and (time.time() - LAST_FETCH_AT < 3600):
        return
    
    try:
        text = await fetch_resume_text()
        if text:
            PROFILE, RESUME_CHUNKS = parse_structured_resume(text)
            build_search_index()
            LAST_FETCH_AT = time.time()
            logger.info(f"Data loaded successfully: {len(RESUME_CHUNKS)} chunks")
        else:
            logger.warning("No resume text fetched")
    except Exception as e:
        logger.error(f"Error ensuring data loaded: {e}")

# ===================== API Routes =====================
@app.get("/")
def home():
    return {
        "service": "enhanced-resume-chatbot", 
        "status": "ready",
        "version": "2.0.0",
        "features": ["intelligent_classification", "interview_simulation", "confidence_scoring"]
    }

@app.get("/health")
async def health():
    try:
        await ensure_data_loaded()
        emb_ready = EMB_MATRIX is not None and hasattr(EMB_MATRIX, "size") and EMB_MATRIX.size > 10
        tfidf_ready = TFIDF_MATRIX is not None and getattr(TFIDF_MATRIX, "shape", [0])[0] > 0
        return {
            "ok": True,
            "chunks": len(RESUME_CHUNKS),
            "profile_loaded": bool(PROFILE.name_en or PROFILE.name_th),
            "vectorizer_ready": tfidf_ready,
            "embeddings_ready": emb_ready,
            "ai_ready": bool(GOOGLE_API_KEY)
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/debug")
async def debug():
    try:
        await ensure_data_loaded()
        return {
            "chunks_count": len(RESUME_CHUNKS),
            "sections": [chunk.section for chunk in RESUME_CHUNKS],
            "profile": {
                "name_en": PROFILE.name_en,
                "name_th": PROFILE.name_th,
                "email": PROFILE.email,
                "skills_count": len(PROFILE.skills)
            },
            "last_fetch": LAST_FETCH_AT,
            "intelligent_features": True
        }
    except Exception as e:
        logger.error(f"Debug endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/debug-metadata")
async def debug_metadata(sample: int = 5):
    """‡∏î‡∏π metadata ‡∏Ç‡∏≠‡∏á‡∏ä‡∏±‡∏á‡∏Å‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏°‡∏µ employer/role/dates/skills ‡πÑ‡∏´‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ index ‡∏à‡∏£‡∏¥‡∏á"""
    try:
        await ensure_data_loaded()
        data = []
        for i, c in enumerate(RESUME_CHUNKS[:sample]):
            data.append({
                "idx": i,
                "section": c.section,
                "metadata": c.metadata,
                "preview": c.content[:120] + ("..." if len(c.content) > 120 else "")
            })
        return {"count": len(RESUME_CHUNKS), "samples": data}
    except Exception as e:
        logger.error(f"debug_metadata failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
@app.post("/refresh")
async def refresh():
    try:
        await ensure_data_loaded(force=True)
        return {
            "ok": True, 
            "chunks": len(RESUME_CHUNKS),
            "message": "Data refreshed successfully"
        }
    except Exception as e:
        logger.error(f"Refresh failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    try:
        await ensure_data_loaded()
        
        question = (req.message or "").strip()
        if not question:
            return ChatResponse(
                reply="üëã ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì Nachapol\n\nüîç **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**\n‚Ä¢ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô: ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£? ‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n‚Ä¢ ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ: ‡∏°‡∏µ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\n‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô: ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á Data Analyst ‡πÑ‡∏´‡∏°?\n‚Ä¢ ‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏á‡∏≤‡∏ô: ‡∏ó‡∏≥‡πÑ‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏µ‡πâ?\n\nüí° ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
                sources=[],
                response_type="greeting",
                confidence=1.0
            )
        
        # Classify the question
        question_class = QuestionClassifier.classify_question(question)
        
        # Try quick answer first for factual questions
        if question_class["type"] == "factual":
            quick = get_quick_answer(question)
            if quick:
                return ChatResponse(
                    reply=quick[0], 
                    sources=[],
                    response_type=quick[1],
                    confidence=0.95
                )
        
        # Retrieve relevant contexts
        hits = multi_query_retrieval(question, question_class, k=6)
        contexts = [hit[1] for hit in hits[:4]]  # Use top 4 for better context
        sources = [(hit[0], hit[1][:200] + "..." if len(hit[1]) > 200 else hit[1]) for hit in hits[:3]]
        
        # Calculate confidence
        confidence = calculate_response_confidence(contexts, question, question_class)
        
        # Generate intelligent response
        reply, response_type = generate_smart_response(question, contexts, question_class)
        
        # Add metadata for transparency
        if response_type in ["mixed", "interview_simulation"]:
            if not any(indicator in reply.lower() for indicator in ["üìã", "üí°", "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏"]):
                # Add transparency note if not already included
                if response_type == "interview_simulation":
                    reply += "\n\n*‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà*"
                elif confidence < 0.6:
                    reply += "\n\n*‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà*"
        
        return ChatResponse(
            reply=reply, 
            sources=sources,
            response_type=response_type,
            confidence=confidence
        )
        
    except Exception as e:
        logger.error(f"Chat endpoint failed: {e}")
        return ChatResponse(
            reply=f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö: {str(e)}\n\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö",
            sources=[],
            response_type="error",
            confidence=0.0
        )

# ===================== Additional API Endpoints =====================
@app.get("/profile")
async def get_profile():
    """Get structured profile data"""
    try:
        await ensure_data_loaded()
        return {
            "name_en": PROFILE.name_en,
            "name_th": PROFILE.name_th,
            "contact": {
                "email": PROFILE.email,
                "phone": PROFILE.phone,
                "location": PROFILE.location
            },
            "skills": PROFILE.skills[:20],  # Top 20 skills
            "sections": {
                section: len([c for c in RESUME_CHUNKS if c.section == section])
                for section in set(chunk.section for chunk in RESUME_CHUNKS)
            }
        }
    except Exception as e:
        logger.error(f"Profile endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/{query}")
async def search_resume(query: str):
    """Search in resume content"""
    try:
        await ensure_data_loaded()
        question_class = QuestionClassifier.classify_question(query)
        hits = multi_query_retrieval(query, question_class, k=5)
        return {
            "query": query,
            "question_type": question_class["type"],
            "results": [
                {
                    "chunk_id": chunk_idx,
                    "section": RESUME_CHUNKS[chunk_idx].section if chunk_idx < len(RESUME_CHUNKS) else "unknown",
                    "content": content[:300] + "..." if len(content) > 300 else content,
                    "score": float(score)
                }
                for chunk_idx, content, score in hits
            ]
        }
    except Exception as e:
        logger.error(f"Search endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze-question")
async def analyze_question(req: ChatRequest):
    """Analyze question type and strategy"""
    try:
        question_class = QuestionClassifier.classify_question(req.message)
        
        return {
            "question": req.message,
            "classification": question_class,
            "strategy": {
                "response_approach": question_class["type"],
                "needs_opinion": question_class["needs_opinion"],
                "interview_mode": question_class["interview_mode"],
                "confidence_threshold": question_class["confidence_threshold"]
            }
        }
    except Exception as e:
        logger.error(f"Question analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/simulate-interview")
async def simulate_interview():
    """Get common interview questions for testing"""
    try:
        await ensure_data_loaded()
        
        interview_questions = [
            "‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏µ‡πâ?",
            "‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
            "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
            "‡∏ó‡πà‡∏≤‡∏ô‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?",
            "‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
            "‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏á‡∏≤‡∏ô?",
            "‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡πÅ‡∏£‡∏á‡∏Å‡∏î‡∏î‡∏±‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?",
            "‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ö‡πâ‡∏≤‡∏á‡πÑ‡∏´‡∏°?"
        ]
        
        # Generate sample responses for a few questions
        sample_responses = {}
        for q in interview_questions[:3]:
            question_class = QuestionClassifier.classify_question(q)
            hits = multi_query_retrieval(q, question_class, k=3)
            contexts = [hit[1] for hit in hits]
            
            if contexts:
                reply, _ = generate_smart_response(q, contexts, question_class)
                sample_responses[q] = reply
        
        return {
            "interview_questions": interview_questions,
            "sample_responses": sample_responses,
            "note": "‡πÉ‡∏ä‡πâ /chat endpoint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå"
        }
        
    except Exception as e:
        logger.error(f"Interview simulation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/capabilities")
async def get_capabilities():
    """Get chatbot capabilities and features"""
    return {
        "question_types": {
            "factual": {
                "description": "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà",
                "examples": ["‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?", "‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?", "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏ö‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô?"],
                "response_style": "‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤ ‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á"
            },
            "opinion": {
                "description": "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô",
                "examples": ["‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á Data Analyst ‡πÑ‡∏´‡∏°?", "‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"],
                "response_style": "‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
            },
            "capability": {
                "description": "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ",
                "examples": ["‡∏°‡∏µ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?", "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô AI ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?"],
                "response_style": "‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå"
            },
            "interview": {
                "description": "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏á‡∏≤‡∏ô",
                "examples": ["‡∏ó‡∏≥‡πÑ‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏µ‡πâ?", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"],
                "response_style": "‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô"
            }
        },
        "features": [
            "‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°",
            "‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô",
            "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏á‡∏≤‡∏ô",
            "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö",
            "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞"
        ],
        "transparency": {
            "factual_responses": "‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô",
            "opinion_responses": "‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå",
            "interview_simulation": "‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á",
            "confidence_scoring": "‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"
        }
    }

# ===================== Testing and Validation =====================
@app.post("/test-intelligence")
async def test_intelligence():
    """Test the intelligent response system"""
    try:
        await ensure_data_loaded()
        
        test_cases = [
            {
                "question": "‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
                "expected_type": "factual",
                "expected_confidence": "high"
            },
            {
                "question": "‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á Data Scientist ‡πÑ‡∏´‡∏°?",
                "expected_type": "opinion",
                "expected_confidence": "medium"
            },
            {
                "question": "‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏µ‡πâ?",
                "expected_type": "interview",
                "expected_confidence": "low-medium"
            },
            {
                "question": "‡∏°‡∏µ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏î‡πâ‡∏≤‡∏ô Machine Learning ‡πÑ‡∏´‡∏°?",
                "expected_type": "capability",
                "expected_confidence": "medium"
            }
        ]
        
        results = []
        for test in test_cases:
            question_class = QuestionClassifier.classify_question(test["question"])
            hits = multi_query_retrieval(test["question"], question_class, k=3)
            contexts = [hit[1] for hit in hits]
            confidence = calculate_response_confidence(contexts, test["question"], question_class)
            
            if contexts:
                reply, response_type = generate_smart_response(test["question"], contexts, question_class)
            else:
                reply, response_type = handle_no_context_response(test["question"], question_class)
            
            results.append({
                "question": test["question"],
                "expected_type": test["expected_type"],
                "actual_type": question_class["type"],
                "type_match": question_class["type"] == test["expected_type"],
                "confidence": confidence,
                "contexts_found": len(contexts),
                "response_preview": reply[:100] + "..." if len(reply) > 100 else reply
            })
        
        return {
            "test_results": results,
            "summary": {
                "total_tests": len(test_cases),
                "type_accuracy": sum(1 for r in results if r["type_match"]) / len(results),
                "avg_confidence": sum(r["confidence"] for r in results) / len(results)
            }
        }
        
    except Exception as e:
        logger.error(f"Intelligence test failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
